\documentclass[%
	corpo=11pt,
    twoside,
    stile=classica,
    oldstyle,
    tipotesi=custom,
    greek,
    evenboxes,
]{toptesi}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amssymb} 
\usepackage{mathtools} 
\usepackage{enumitem}
\usepackage{array}
\usepackage{booktabs}

\usepackage{pifont}
\usepackage{fontawesome}
\usepackage[utf8]{inputenc}  % Supporto per caratteri UTF-8
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{listings}

\definecolor{green}{RGB}{34,139,34}
\definecolor{red}{RGB}{220,20,60}
\definecolor{blue}{RGB}{30,144,255}

\usepackage{hyperref}
\hypersetup{%
    pdfpagemode={UseOutlines},
    bookmarksopen,
    pdfstartview={FitH},
    colorlinks,
    linkcolor={blue},
    citecolor={blue},
    urlcolor={blue}
  }

%%%%%%% use PDFLATEX 

\usepackage{lipsum} %to insert random text

\usepackage{geometry} %for the margins
\newcommand\fillin[1][4cm]{\makebox[#1]{\dotfill}} %for the dotted line in the frontispiace

\usepackage{dcolumn}
\newcolumntype{d}{D{.}{.}{-1} } %to vetical align numbers in tables, along the decimal dot

%\usepackage{amsmath}

\usepackage{natbib} % for the bibliography


%%%%%%% Local definitions
\newtheorem{osservazione}{Osservazione}% Standard LaTeX
\newtheorem{observation}{Observation}% Standard LaTeX


%%%%%%% Custom fonts for title page
\newcommand\customfont[1]{{\usefont{T1}{Poppins-Regular}{m}{n} #1 }}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}\errorcontextlines=9
%\english

\input{./title.tex} %the frontispiece

%%%%%%% Dedication
%\ifclassica%
%{\begin{dedica}
    %A mio padre

    %\textdagger\ A mio nonno Pino
%\end{dedica}
%%%%%%% 

\sommario%summary
%Here goes the abstrat of your thesis
Questa tesi esplora il ruolo delle copule nella modellazione della dipendenza tra variabili casuali, con particolare attenzione alle applicazioni in finanza. Dopo un'introduzione ai concetti fondamentali delle copule e alla loro teoria matematica, vengono analizzate diverse famiglie di copule, le loro proprietà e il loro utilizzo nella gestione del rischio finanziario. Successivamente, si discutono i metodi di stima dei parametri delle copule, con un focus su metodi classici e bayesiani, e si presenta un’implementazione pratica sui dati finanziari del DAX. Infine, vengono confrontati i risultati ottenuti con diverse copule e si discutono le implicazioni per la gestione del rischio e l’ottimizzazione di portafoglio.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ringraziamenti%acknowledgements
%Acknowledge the people you love and/or work with
Da scrivere

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\tablespagetrue\figurespagetrue%to include the list of tables
%and the list of figures - yuo can comment these commands

\indici%table of content
%It automatically generated

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Citation
%If you feel like a poetic guy!
%\ifclassica   
%\begin{citazioni}
 %   \textit{If you cannot understand my\\argument, and declare}\\
  %  it's Greek to me\\
   % \textit{you are quoting Shakespeare.}
    
    %[\textsc{B. Levin}, Quoting Shakespeare]\vspace{1em}
%\end{citazioni}
%\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\mainmatter

\part{Fondamenti delle Copule}

\chapter{Introduzione}

\section{Definizione e motivazione dello studio}
Le copule sono strumenti matematici che permettono di modellare e stimare la dipendenza tra diverse variabili casuali. Sono particolarmente utili in finanza, dove la dipendenza tra i rendimenti degli asset, i tassi di interesse e i tempi di default sono fattori cruciali per la valutazione del rischio e la determinazione del prezzo di strumenti finanziari complessi. L’importanza delle copule risiede nella loro capacità di separare la modellazione delle distribuzioni marginali delle singole variabili dalla modellazione della loro struttura di dipendenza.

In altre parole, invece di dover specificare una funzione di distribuzione congiunta per tutte le variabili, è possibile utilizzare una copula per combinare le distribuzioni marginali di ciascuna variabile in una distribuzione congiunta che rifletta la dipendenza desiderata. Questo approccio offre grande flessibilità nella modellazione, poiché consente di scegliere le distribuzioni marginali e la copula in modo indipendente, a seconda delle caratteristiche specifiche dei dati e del problema in esame.

Ad esempio, si potrebbe utilizzare una distribuzione t di Student per modellare i rendimenti degli indici azionari, che spesso presentano code più spesse rispetto alla distribuzione normale, e quindi utilizzare una copula di Gumbel per rappresentare la dipendenza asimmetrica tra i mercati, con una maggiore probabilità di movimenti congiunti al rialzo rispetto a quelli al ribasso.

La teoria delle copule si basa sul teorema di Sklar, che afferma che ogni funzione di distribuzione congiunta può essere espressa in termini di una copula e delle distribuzioni marginali delle variabili. Il teorema di Sklar garantisce l’esistenza e l’unicità della copula nel caso di variabili casuali continue. 

Esistono diverse famiglie di copule, ciascuna con proprietà specifiche in termini di dipendenza di coda, simmetria e altre caratteristiche. Alcune delle famiglie di copule più utilizzate in finanza includono la copula gaussiana, la copula t di Student, le copule Archimedee (come la copula di Gumbel, la copula di Clayton e la copula di Frank) e la copula di Marshall-Olkin. La scelta della copula più adatta dipende dalla natura del problema e dalle caratteristiche della dipendenza che si desidera modellare.

Ad esempio, la
copula t di Student é spesso preferita alla copula gaussiana quando si vogliono
modellare dipendenze di coda più
elevate, mentre le copule Archimedee consentono di modellare diversi tipi di dipendenza asimmetrica.

Le copule trovano applicazione in diversi ambiti della finanza, tra cui:

\begin{itemize}
	\item \textbf{Pricing di opzioni multivariate e altri derivati}: le copule possono essere utilizzate per modellare la dipendenza tra i sottostanti di un’opzione basket, un’opzione rainbow o altri derivati multi-asset, consentendo una valutazione più accurata del prezzo di questi strumenti.
	
	\item \textbf{Gestione del rischio}: le copule sono ampiamente utilizzate nella modellazione del rischio di credito, dove consentono di stimare la probabilità di default congiunta di diverse attività o controparti. Le copule sono anche utilizzate nella stima del Value at Risk (VaR) di portafogli contenenti attività con distribuzioni non normali e dipendenze complesse.
	
	\item \textbf{Calibrazione e simulazione}: la flessibilità delle copule consente di calibrare i modelli ai dati di mercato in modo efficiente e di simulare scenari di mercato realistici che tengano conto della dipendenza tra le variabili.
\end{itemize}

In sintesi, le copule rappresentano uno strumento matematico versatile e potente per la modellazione della dipendenza in finanza, con un ampio spettro di applicazioni pratiche nella valutazione del rischio, nel pricing di derivati e nella gestione del portafogli.


\chapter{Fondamenti Matematici delle Copule}

\section{Definizione di copula e Teorema di Sklar}

Le copule sono strumenti matematici che permettono di modellare e rappresentare la dipendenza tra variabili casuali. A differenza di misure di dipendenza tradizionali come la correlazione lineare, le copule catturano la dipendenza in modo più completo, includendo la dipendenza nelle code delle distribuzioni e non limitandosi a relazioni lineari.

Ecco una spiegazione delle formule e delle proprietà chiave:

\subsection{Definizione di Copula}
Una \(d\)-copula è una funzione \( C : [0,1]^d \to [0,1] \), dove \( d \geq 2 \) (numero di variabili; nelle proprietà seguenti consideriamo le copule bivariate), che soddisfa le seguenti proprietà:

\begin{enumerate}
	\item \textbf{Groundedness:}  
	\[
	C(u,0) = C(0,v) = 0, \quad \forall u, v \in [0,1]^2
	\]
	Ciò significa che la copula è zero se una delle variabili è zero.
	
	\item \textbf{Marginalità:}  
	\[
	C(u,1) = u, \quad C(1,v) = v, \quad \forall u, v \in [0,1]^2
	\]
	Questa proprietà assicura che la copula sia coerente con le distribuzioni marginali, ovvero che quando una delle variabili assume il suo valore massimo, la copula coincida con la funzione di ripartizione dell’altra variabile.
	
	\item \textbf{2-crescita (o 2-increasing):}  
	\[
	C(u_2,v_2) - C(u_2,v_1) - C(u_1,v_2) + C(u_1,v_1) \geq 0, \quad \forall u_1 \leq u_2, v_1 \leq v_2 \in [0,1]^2
	\]
	Questa proprietà assicura che la copula sia non decrescente in entrambe le variabili, il che è necessario affinché la copula rappresenti una dipendenza positiva o negativa tra le variabili.
\end{enumerate}

\subsection{Teorema di Sklar}
Questo teorema, centrale nella teoria delle copule, stabilisce un legame tra le copule e le funzioni di distribuzione congiunta.  

In breve, il teorema afferma che:  
Data una funzione di distribuzione congiunta \( F(x,y) \) con marginali \( F_1(x) \) e \( F_2(y) \), esiste una copula \( C \) tale che:
\[
F(x,y) = C(F_1(x), F_2(y))
\]
Inoltre, se \( F_1(x) \) e \( F_2(y) \) sono continue, allora la copula \( C \) è unica.

\subsubsection{Conseguenze del Teorema di Sklar}
\begin{itemize}
	\item \textbf{Costruzione di modelli di dipendenza:}  
	Permette di costruire una funzione di distribuzione congiunta a partire da distribuzioni marginali arbitrarie e da una copula che ne modella la dipendenza. Questa proprietà è particolarmente utile per modellare dati reali, dove spesso si conoscono le distribuzioni marginali ma non la struttura di dipendenza.
	
	\item \textbf{Separazione tra marginali e dipendenza:}  
	Mette in luce come la struttura di dipendenza tra le variabili sia completamente catturata dalla copula, indipendentemente dalle distribuzioni marginali.
\end{itemize}


\section{Famiglie principali di copule (Gaussiane, t-Student, Archimedee) }

\subsection{Famiglie di copule}
Esistono diverse famiglie di copule, classificate in base alla loro struttura o ai metodi utilizzati per la loro costruzione. Di seguito, vengono elencate alcune delle principali famiglie:

\begin{itemize}
	\item \textbf{Fréchet-Hoeffding:} Questa famiglia include le copule che rappresentano i limiti inferiore (\(W\)) e superiore (\(M\)) della dipendenza tra due variabili casuali. La copula \(W\) rappresenta la perfetta dipendenza negativa, mentre la copula \(M\) rappresenta la perfetta dipendenza positiva.
	
	\item \textbf{Cuadras-Augé:} Questa famiglia di copule è costruita come una media geometrica ponderata delle copule \(M\) e \(P\), dove \(P\) rappresenta l’indipendenza tra le variabili.
	
	\item \textbf{Marshall-Olkin:} Questa famiglia di copule è spesso utilizzata per modellare la dipendenza tra variabili casuali che rappresentano tempi di vita.
	
	\item \textbf{Archimedee:} Queste copule sono generate da una funzione detta "generatore". Le copule Archimedee sono popolari per la loro flessibilità e la relativa facilità di utilizzo.
\end{itemize}

\subsection{Proprietà delle copule}
Le copule possiedono diverse proprietà che le rendono utili per la modellazione della dipendenza. Alcune di queste proprietà sono:

\begin{itemize}
	\item \textbf{Invarianza rispetto a trasformazioni monotone crescenti:}  
	Le copule sono invarianti rispetto a trasformazioni strettamente crescenti delle variabili marginali.
	
	\item \textbf{Misure di concordanza:}  
	Diverse misure di concordanza come la rho di Spearman e la tau di Kendall possono essere espresse in termini di copule.
	
	\item \textbf{Dipendenza di coda:}  
	Le copule possono catturare la dipendenza tra le code delle distribuzioni marginali, ovvero la tendenza delle variabili ad assumere valori estremi congiuntamente.
\end{itemize}

Possiamo quindi affermare che le copule offrono un approccio potente e flessibile per la modellazione della dipendenza tra variabili casuali. La loro capacità di separare la struttura di dipendenza dalle distribuzioni marginali, la loro invarianza rispetto a trasformazioni monotone crescenti e la loro capacità di catturare la dipendenza di coda le rendono strumenti preziosi in molte applicazioni pratiche.




\part{Applicazioni Finanziarie delle Copule}

\chapter{Copule e Gestione del Rischio Finanziario}

\section{Superamento della correlazione lineare nelle distribuzioni non normali}

L'assunzione di normalità dei rendimenti, tipico di modelli come quello di Black-Scholes, è spesso disatteso nei mercati finanziari. Le serie storiche di strumenti come azioni ed obbligazioni dimostrano la presenza di code più pesanti rispetto a quanto previsto dalla distribuzione normale e la diffusione di prodotti derivati con payoff non lineari amplifica ulteriormente questo fenomeno.

In questo contesto, la \textbf{correlazione lineare}, misurata ad esempio con il coefficiente di Pearson, si dimostra uno strumento limitato. Essa cattura solo le dipendenze lineari tra le variabili, mentre le relazioni tra gli asset finanziari possono assumere forme ben più complesse. La correlazione lineare è efficace solo quando le variabili sono legate da relazioni lineari. Tuttavia, in presenza di legami non lineari, la correlazione lineare potrebbe essere fuorviante. Ad esempio, una variabile con distribuzione chi-quadrato è perfettamente correlata al suo quadrato, che ha distribuzione normale, ma la correlazione lineare non sarebbe in grado di rappresentare correttamente questa relazione.

\subsection{Coefficiente di Pearson}

Il coefficiente di correlazione lineare, noto anche come correlazione di Pearson, è una misura della dipendenza lineare tra due variabili casuali che assumono valori reali e che hanno varianza finita. È definito come la covarianza delle due variabili divisa per il prodotto delle loro deviazioni standard. Formalmente, per due variabili casuali non degeneri \( X \) e \( Y \) appartenenti a \( L^2 \), il coefficiente di correlazione lineare \( \rho_{XY} \) è:

\[
\rho_{XY} = \frac{\text{cov}(X,Y)}{\sqrt{\text{var}(X)\text{var}(Y)}}
\]
\newline
Il coefficiente di correlazione di Pearson assume valori compresi tra -1 e +1, dove:
\begin{itemize}
	\item \( +1 \) indica una perfetta correlazione lineare positiva: all’aumentare di una variabile, l’altra aumenta in modo perfettamente lineare.
	\item \( -1 \) indica una perfetta correlazione lineare negativa: all’aumentare di una variabile, l’altra diminuisce in modo perfettamente lineare.
	\item \( 0 \) indica l’assenza di correlazione lineare: non c’è una relazione lineare tra le due variabili.
\end{itemize}

È importante sottolineare che il coefficiente di correlazione di Pearson misura solo la dipendenza lineare. Due variabili possono essere fortemente dipendenti in modo non lineare e avere comunque un coefficiente di correlazione di Pearson pari a zero.

\subsection{Vantaggi delle Copule rispetto alla Correlazione Lineare}
Le \textbf{copule}, invece, offrono un approccio più flessibile per modellare la dipendenza tra variabili casuali, anche in presenza di distribuzioni non normali. Il vantaggio principale delle copule risiede nella loro capacità di separare la struttura di dipendenza dalle distribuzioni marginali. Questo permette di combinare diverse distribuzioni marginali, capaci di cogliere la non-normalità dei rendimenti (ad esempio la distribuzione t di Student o distribuzioni asimmetriche), con una vasta gamma di copule che descrivono la struttura di dipendenza.

Il \textbf{Teorema di Sklar}, fondamento della teoria delle copule, afferma che ogni funzione di distribuzione congiunta può essere espressa in termini di distribuzioni marginali e di una copula. Questa proprietà permette di costruire modelli di dipendenza altamente flessibili, adatti a rappresentare le complesse relazioni tra gli asset finanziari. Ad esempio, è possibile utilizzare una copula gaussiana per modellare la struttura di dipendenza, pur mantenendo distribuzioni marginali non gaussiane per i singoli asset. In questo modo, si ottiene un modello in grado di catturare sia la non-normalità dei rendimenti sia le strutture di dipendenza tra gli stessi.

In definitiva, le copule rappresentano uno strumento più completo e affidabile rispetto alla correlazione lineare per modellare le dipendenze tra variabili casuali, soprattutto in presenza di distribuzioni non normali. La loro flessibilità e capacità di rappresentare accuratamente le complesse relazioni tra gli asset le rendono essenziali per una corretta valutazione del rischio, un pricing accurato e una migliore comprensione delle dinamiche dei mercati finanziari.
\newpage
\section{Dipendenza di coda e impatti su Value-at-Risk (VaR) e Expected Shortfall}

La dipendenza di coda si riferisce alla tendenza di due o più variabili casuali a muoversi insieme in modo più estremo nelle code delle loro distribuzioni, rispetto a quanto previsto da una distribuzione normale con la stessa correlazione lineare. In altre parole, la dipendenza di coda misura la probabilità che si verifichino eventi estremi congiuntamente.

Sappiamo che la non normalità a livello univariato è associata al cosiddetto problema della \textit{fat-tail}. In un contesto multivariato, il problema della \textit{fat-tail} può essere riferito sia alle distribuzioni marginali univariate che alle distribuzioni congiunte di probabilità di grandi movimenti di mercato. Questo concetto è chiamato \textbf{tail dependence}. L’uso di funzioni copula ci permette di modellare separatamente queste due caratteristiche. Per rappresentare la dipendenza dalla coda consideriamo la probabilità che un evento con probabilità inferiore a \( v \) si verifichi nella prima variabile, dato che un evento con probabilità inferiore a \( v \) si verifica nella seconda. In concreto, ci chiediamo quale sia la probabilità di osservare, ad esempio, un crollo con probabilità inferiore di \( v=1\% \) nell’indice Nikkei 225, dato che nell’indice S\&P 500 si è verificato un crollo con probabilità inferiore all’1\%. Si ha:

\[
\lambda(v) \equiv \Pr(Q_{NKY} \leq v \mid Q_{SP} \leq v)
\]

\[
= \frac{\Pr(Q_{NKY} \leq v, Q_{SP} \leq v)}{\Pr(Q_{SP} \leq v)}
\]

\[
= \frac{C(v,v)}{v}
\]

\subsection{Tipi di dipendenza di coda}
Dopo che abbiamo calcolato il nostro \( \lambda(v) \), possiamo dividere la dipendenza di coda in due tipi principali:

\begin{itemize}
	\item \textbf{Dipendenza di coda inferiore}: misura la probabilità che entrambe le variabili assumano valori estremamente bassi contemporaneamente.
	\[
	\lambda_L \equiv \lim_{v \to 0^+} \frac{C(v,v)}{v}
	\]
	
	\item \textbf{Dipendenza di coda superiore}: misura la probabilità che entrambe le variabili assumano valori estremamente alti contemporaneamente.
	\[
	\lambda_U = \lim_{v \to 1^-} \lambda_v \equiv \lim_{v \to 1^-} \frac{\Pr(Q_{NKY} > v, Q_{SP} > v)}{\Pr(Q_{SP} > v)}
	\]
	
	\[
	= \lim_{v \to 1^-} \frac{1 - 2v + C(v,v)}{1 - v}
	\]
\end{itemize}

\subsection{Importanza nel VaR e nell’Expected Shortfall}
La dipendenza di coda ha un impatto significativo sul calcolo del \textbf{VaR} e dell’\textbf{Expected Shortfall}, due misure di rischio ampiamente utilizzate nella gestione del rischio finanziario.

\begin{itemize}
	\item \textbf{VaR (Value at Risk)}: rappresenta la perdita massima stimata che un portafoglio potrebbe subire in un determinato orizzonte temporale e con un dato livello di confidenza.
	\item \textbf{Expected Shortfall}: rappresenta la perdita media attesa in caso di superamento del VaR.
\end{itemize}

In presenza di dipendenza di coda, il VaR e l’Expected Shortfall calcolati assumendo una distribuzione normale tendono a sottostimare il rischio effettivo del portafoglio. Questo perché la distribuzione normale non riesce a catturare adeguatamente la probabilità di eventi estremi congiunti. Mentre utilizzando le copule per modellare la dipendenza tra gli asset di un portafoglio, è possibile ottenere una stima più accurata del VaR e dell’Expected Shortfall, tenendo conto della probabilità di eventi estremi congiunti. Quindi grazie alle copule è possibile una misura più accurata del rischio di portafoglio e si possono prendere decisioni più consapevoli.
\newpage
\section{Pricing di derivati multivariati con copule e gestione del rischio}

\subsection{Tariffare le opzioni}
La valutazione di opzioni multivariate, come le opzioni basket o rainbow, che dipendono da più attività sottostanti, rappresenta una sfida significativa in finanza. Le copule forniscono un potente strumento per affrontare questo problema.

\begin{itemize}
	\item In sostanza, una copula viene utilizzata per costruire la distribuzione congiunta dei prezzi delle attività sottostanti alla scadenza.
	\item Questa distribuzione viene quindi utilizzata per calcolare il valore atteso del payoff dell’opzione in base a tutti i possibili risultati dei prezzi delle attività sottostanti.
	\item Attualizzando questo valore atteso al tasso privo di rischio, si ottiene il prezzo dell’opzione.
\end{itemize}

Nei mercati incompleti, dove non esiste una misura di probabilità unica priva di arbitraggio, le copule sono fondamentali per derivare strategie di super-replicazione. Queste strategie mirano a creare un portafoglio di attività negoziabili che replichi il payoff dell’opzione in tutte le possibili situazioni future, garantendo così l’assenza di opportunità di arbitraggio. Le copule consentono di determinare i limiti superiori e inferiori del prezzo dell’opzione in base alle diverse ipotesi sulla struttura di dipendenza tra le attività sottostanti. Mostriamo ora degli esempi:

\begin{itemize}
	\item \textbf{Opzioni arcobaleno}: queste opzioni, che dipendono dal minimo o dal massimo di un paniere di attività, possono essere valutate utilizzando le copule per catturare la dipendenza tra i rendimenti delle attività. Le fonti dimostrano come le copule possano essere utilizzate per derivare strategie di super-replicazione per le opzioni arcobaleno, fornendo limiti superiori e inferiori al prezzo.
	
	\item \textbf{Opzioni barriera}: per le opzioni in cui l’esercizio è condizionato al fatto che il prezzo dell’attività sottostante raggiunga o meno una determinata barriera, le copule possono essere utilizzate per modellare la dipendenza tra il processo del prezzo dell’attività e l’evento di attivazione della barriera.
\end{itemize}

\subsection{Gestione dei rischi}
Le copule possono essere utilizzate per modellare la dipendenza tra diversi tipi di rischio, come rischio di mercato, rischio di credito e rischio operativo. Ciò è particolarmente utile per le istituzioni finanziarie che sono esposte a più tipi di rischio, in quanto consente loro di valutare il rischio complessivo a cui sono esposte. Ad esempio, una banca può utilizzare le copule per modellare la dipendenza tra le insolvenze sui prestiti e i movimenti dei tassi di interesse, consentendo loro di valutare il rischio del proprio portafoglio prestiti in diversi scenari economici.

In particolare, nella gestione del rischio di credito, le copule vengono utilizzate nella valutazione di strumenti di debito strutturati come le obbligazioni garantite da crediti (CDO). Le CDO sono obbligazioni garantite da un pool di attività sottostanti, come mutui o prestiti alle imprese. Il rischio di credito di una CDO dipende dalla dipendenza tra le insolvenze delle attività sottostanti. Le copule forniscono un modo flessibile per modellare questa dipendenza, consentendo agli investitori di valutare il rischio e il rendimento delle CDO in modo più accurato.

\section{Modelli di copula}

\subsection{Tipi di modelli di copula}
\begin{itemize}
	\item \textbf{Copula gaussiana}: descrive la dipendenza tra variabili casuali utilizzando la distribuzione normale multivariata. Non è in grado di catturare la dipendenza dalla coda. È definita come la funzione di distribuzione congiunta di un vettore normale multivariato standard, dove ogni variabile marginale è stata trasformata nella sua forma univariate standard utilizzando la funzione di distribuzione normale standard inversa.
	
	\[
	C_R^{Ga}(u,v) = \Phi_R(\Phi^{-1}(u), \Phi^{-1}(v))
	\]
	
	\[
	= \int_{-\infty}^{\Phi^{-1}(u)} \int_{-\infty}^{\Phi^{-1}(v)} \frac{1}{2\pi \sqrt{1-\rho_{XY}^2}} \exp \left( \frac{2\rho_{XY}st - s^2 - t^2}{2(1-\rho_{XY}^2)} \right) ds \, dt
	\]
	
	\item \textbf{Copula t di Student}: Questa copula può catturare la dipendenza dalla coda e viene spesso utilizzata per modellare i rendimenti degli asset che mostrano code pesanti. La copula t di Student bivariata è data dalla seguente formula:
	
	\[
	C_t(u,v) = \int_{-\infty}^{t_{\nu}^{-1}(u)} \int_{-\infty}^{t_{\nu}^{-1}(v)} \frac{\Gamma \left( \frac{\nu+2}{2} \right)}{\Gamma \left( \frac{\nu}{2} \right) \pi \nu \sqrt{1-\rho^2}} \left(1 + \frac{x^2 -2\rho xy + y^2}{\nu(1-\rho^2)}\right)^{-\frac{\nu+2}{2}} dx \, dy
	\]
	
	dove \( t_{\nu}^{-1} \) è l’inversa della funzione di distribuzione t di Student univariata con \( \nu \) gradi di libertà e \( \rho \) è il coefficiente di correlazione.
	
	\item \textbf{Copula di Clayton}: questa copula mostra una forte dipendenza dalla coda inferiore, il che significa che le variabili hanno maggiori probabilità di assumere insieme valori estremi bassi. È esaustiva e fornisce la copula del prodotto se \( \alpha = 0 \), il limite inferiore di Fréchet \( \max(v+z-1, 0) \) quando \( \alpha = -1 \) e quello superiore per \( \alpha \to +\infty \). È definita dalla seguente formula:
	
	\[
	C(v,z) = \max[(v^{-\alpha} + z^{-\alpha} - 1)^{-\frac{1}{\alpha}}, 0]
	\]
	
	\item \textbf{Copula di Gumbel}: questa copula mostra una forte dipendenza dalla coda superiore, indicando che le variabili hanno maggiori probabilità di assumere insieme valori estremi elevati. Fornisce la copula del prodotto se \( \alpha = 1 \) e il limite superiore di Fréchet \( \min(v,z) \) per \( \alpha \to +\infty \). È definita dalla seguente formula:
	
	\[
	C(v,z) = \exp \left( -[(-\ln v)^\alpha + (-\ln z)^\alpha]^{\frac{1}{\alpha}} \right)
	\]
	
	\item \textbf{Copula di Frank}: questa copula è una copula simmetrica che può catturare sia la dipendenza dalla coda superiore che quella dalla coda inferiore. Si riduce alla copula del prodotto se \( \alpha = 0 \) e raggiunge i limiti inferiori e superiori di Fréchet rispettivamente per \( \alpha \to -\infty \) e \( \alpha \to +\infty \). È definita dalla seguente formula:
	
	\[
	C(v,z) = -\frac{1}{\alpha} \ln \left( 1 + \frac{(e^{-\alpha v} - 1)(e^{-\alpha z} - 1)}{e^{-\alpha} - 1} \right)
	\]
	
\end{itemize}




\part{Preparazione dei Dati e Assunzioni}

\chapter{Preparazione dei Dati per la Modellazione con Copule}

\section{Struttura e caratteristiche dei dati}

Il seguente dataset contiene dati storici sul DAX, che rappresentano prezzi o indici di mercato rilevanti. La raccolta dati fornisce i valori di Open, High, Low, Close partendo dal giorno 02/01/2020 ore 01:15:00 e terminando con il giorno 03/03/2022 ore 09:14:00. La raccolta e la gestione adeguata di tali dati sono fondamentali per analizzare le dipendenze tra strumenti finanziari tramite modelli di copula. Tramite questi dati calcoleremo i rendimenti giornalieri, un passaggio necessario per la modellazione delle dipendenze.

\begin{table}[h]
	\centering
	\renewcommand{\arraystretch}{1.2} % Spaziatura tra righe
	\setlength{\tabcolsep}{6pt} % Spaziatura tra colonne
	\small % Riduci leggermente la dimensione del testo per adattare meglio la tabella
	\begin{tabular}{lcccc} % Colonne: l (sinistra), c (centrate)
		\toprule
		\textbf{DateTime} & \textbf{Open} & \textbf{High} & \textbf{Low} & \textbf{Close} \\
		\midrule
		02/01/2020 01:15:00 +01:00 & 13174 & 13194.5 & 13171.5 & 13177.5 \\
		02/01/2020 01:16:00 +01:00 & 13177 & 13185 & 13177 & 13180.5 \\
		02/01/2020 01:17:00 +01:00 & 13180.5 & 13183 & 13179 & 13181.5 \\
		02/01/2020 01:18:00 +01:00 & 13181.5 & 13182 & 13180.5 & 13182 \\
		02/01/2020 01:19:00 +01:00 & 13182 & 13183 & 13180.5 & 13181.5 \\
		\midrule
		\dots & \dots & \dots & \dots & \dots \\
		\midrule
		03/03/2022 09:10:00 +01:00 & 14019 & 14031 & 14010 & 14013 \\
		03/03/2022 09:11:00 +01:00 & 14013 & 14019 & 14010 & 14000 \\
		03/03/2022 09:12:00 +01:00 & 13999 & 14013 & 13996 & 14006 \\
		03/03/2022 09:13:00 +01:00 & 14007 & 14015 & 13995 & 14009 \\
		03/03/2022 09:14:00 +01:00 & 14009 & 14020 & 14009 & 14020 \\
		\bottomrule
	\end{tabular}
	\caption{616397 osservazioni raccolte di open, high, low and close }
	\label{tab:dati_finanziari}
\end{table}

\section{Pulizia e preprocessing}

I dati finanziari spesso includono anomalie come valori mancanti o outlier che devono essere gestiti prima dell’analisi. Saranno implementate le seguenti tecniche di pulizia dei dati:

\begin{itemize}
	\item \textbf{Gestione dei valori mancanti}: rimuovere eventuali righe con valori mancanti per evitare distorsioni.
	\item \textbf{Gestione degli outlier}: utilizzare tecniche di filtraggio per identificare ed eliminare gli outlier, assicurando che l’analisi si concentri sui valori centrali più rappresentativi.
\end{itemize}

\textbf{Outlier}: nel contesto dei dati azionari, un outlier (o valore anomalo) è un’osservazione che si discosta significativamente dalla norma o dalla tendenza generale del dataset. In altre parole, si tratta di un dato che è molto diverso rispetto agli altri valori presenti nel campione.\\

Il motivo per cui vanno eliminati è che possono distorcere le analisi statistiche, poiché influenzano la media, la deviazione standard e altre misure di dispersione dei dati.\\

Ecco alcune righe di codice utilizzate per ``pulire'' i dati:
\begin{verbatim}
	import pandas as pd
	
	# Load the data
	data = pd.read_csv('DAX_3Y-1M.csv',index_col='DateTime',
	parse_dates=True)
	
	# Drop rows with missing values
	data=data.dropna()
	
	# Verifica e gestione degli outlier tramite interquartile
	range (IQR)
	Q1 = data.quantile(0.25)
	Q3 = data.quantile(0.75)
	IQR = Q3 - Q1
	filtered_data = data[~((data < (Q1 - 1.5 * IQR)) | 
	(data > (Q3 + 1.5 * IQR))).any(axis=1)]
\end{verbatim}

Circa 6000 righe di dati sono state eliminate da questo processo.
\newpage
\section{Trasformazione dei dati}

Dopo la pulizia, è necessario trasformare i dati per ottenere una scala uniforme. Poiché i modelli di copula richiedono margini uniformi, trasformeremo i dati in rendimenti logaritmici per ottenere stazionarietà e calcoleremo i punteggi standardizzati:

Ecco alcune righe di codice utilizzate per ``trasformare'' i dati:

\begin{verbatim}
	import numpy as np
	
	# Calcolo dei rendimenti logaritmici
	log_returns = np.log(filtered_data / filtered_data.shift(1)).dropna()
	
	# Standardizzazione dei dati
	# (sottraggo la media e divido per la deviazione standard)
	standardized_data = (log_returns - log_returns.mean()) / log_returns.std()
\end{verbatim}

\begin{table}[h]
	\centering
	\renewcommand{\arraystretch}{1.2} % Spaziatura tra righe
	\setlength{\tabcolsep}{6pt} % Spaziatura tra colonne
	\small % Riduci leggermente la dimensione del testo per adattare meglio la tabella
	\begin{tabular}{lcccc} % Colonne: l (sinistra), c (centrate)
		\toprule
		\textbf{DateTime} & \textbf{Open} & \textbf{High} & \textbf{Low} & \textbf{Close} \\
		\midrule
		02/01/2020 01:16:00 +01:00 & 0.513283 & -1.507703 & 0.853297 & 0.498567 \\
		02/01/2020 01:17:00 +01:00 & 0.598720 & -0.555875 & 0.310070 & 0.166015 \\
		02/01/2020 01:18:00 +01:00 & 0.170871 & 0.079182 & 0.154914 & 0.028292 \\
		02/01/2020 01:19:00 +01:00 & 0.085317 & 0.158563 & 0.077349 & -0.083337 \\
		02/01/2020 01:20:00 +01:00 & -0.085772 & 0.237926 & 0.154896 & 0.498415 \\
		\midrule
		\dots & \dots & \dots & \dots & \dots \\
		\midrule
		03/03/2022 09:10:00 +01:00 & -1.447601 & -0.000208 & -0.437935 & -1.250808 \\
		03/03/2022 09:11:00 +01:00 & -0.965660 & -1.791008 & -1.459979 & -2.033947 \\
		03/03/2022 09:12:00 +01:00 & -2.254511 & -0.149511 & -0.146421 & 0.938266 \\
		03/03/2022 09:13:00 +01:00 & 1.288211 & -0.448180 & -0.584449 & 0.469065 \\
		03/03/2022 09:14:00 +01:00 & 0.321767 & 0.746358 & 2.043913 & 1.719639 \\
		\bottomrule
	\end{tabular}
	\caption{Rendimenti logaritmici standardizzati}
	\label{tab:dati_finanziari_modificati}
\end{table}

\newpage
\section{Normalizzazione}

Per applicare correttamente i modelli di copula, i dati devono essere trasformati in una distribuzione uniforme sull’intervallo \([0,1]\). Questo passaggio permette ai dati di adattarsi meglio alla funzione di copula che verrà utilizzata per modellare le dipendenze:

\begin{verbatim}
	from scipy.stats import norm
	
	# Normalizzazione tramite la funzione di distribuzione cumulativa (CDF)
	uniform_data = norm.cdf(standardized_data)
\end{verbatim}

\begin{table}[h]
	\centering
	\renewcommand{\arraystretch}{1.2} % Spaziatura tra righe
	\setlength{\tabcolsep}{6pt} % Spaziatura tra colonne
	\small % Riduci leggermente la dimensione del testo per adattare meglio la tabella
	\begin{tabular}{cccc} % Quattro colonne centrate
		\toprule
		\textbf{Open} & \textbf{High} & \textbf{Low} & \textbf{Close} \\
		\midrule
		0.69612324 & 0.06581532 & 0.80325258 & 0.69095767 \\
		0.7253202  & 0.28914825 & 0.6217461  & 0.56592753 \\
		0.56783755 & 0.53155597 & 0.56155544 & 0.53303114 \\
		\midrule
		\dots & \dots & \dots & \dots \\
		\midrule
		0.01288201 & 0.44057533 & 0.44186549 & 0.82604578 \\
		0.90116376 & 0.32701177 & 0.279459   & 0.68048837 \\
		0.62618555 & 0.7722743  & 0.97951894 & 0.95725093 \\
		\bottomrule
	\end{tabular}
	\caption{Dati uniformati all'intervallo [0,1]}
	\label{tab:dati_array}
\end{table}

\section{Assunzioni nei modelli di copula}

Per l’uso corretto dei modelli di copula, è essenziale discutere alcune assunzioni chiave:

\begin{itemize}
	\item \textbf{Uniformità marginale}: l’assunzione primaria nella modellazione delle copule è che le variabili marginali abbiano distribuzioni uniformi. Abbiamo utilizzato la funzione di distribuzione cumulativa per garantire questa uniformità.
	
	\item \textbf{Struttura di dipendenza}: i modelli di copula modellano la struttura di dipendenza tra le variabili senza fare ipotesi sui margini. Ciò significa che, dopo la trasformazione, possiamo utilizzare diversi tipi di copule per analizzare come i vari strumenti finanziari si muovono insieme.
	
	\item \textbf{Stazionarietà}: è importante che i dati siano stazionari, ovvero che le loro proprietà statistiche (come media e varianza) siano costanti nel tempo. Abbiamo utilizzato la differenziazione logaritmica per rendere i dati stazionari.
	
	\item \textbf{Normalità}: per l’utilizzo di una copula Gaussiana, i margini devono approssimare la normalità. Sebbene non sia strettamente necessario per altre copule come la t-Copula, una trasformazione per avvicinarsi alla normalità può essere utile per semplificare l’analisi.
\end{itemize}

\section{Distribuzione dei rendimenti}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{./Pictures/rendimenti.png}
	\caption{Distribuzione dei Rendimenti Logaritmici del DAX (Subset)}
	\label{fig:log_returns_distribution}
\end{figure}

Ecco la distribuzione dei rendimenti logaritmici del DAX, calcolata su un subset dei dati disponibili. La distribuzione mostra la tipica forma a campana, con alcune code più pesanti, suggerendo la presenza di eventi estremi più frequenti rispetto a una normale distribuzione Gaussiana. Questa caratteristica supporta l’idea di utilizzare copule come la Student-t, che meglio cattura queste dipendenze nelle code.\\


Di seguito la matrice di correlazione tra i rendimenti logaritmici delle diverse colonne di prezzo (Open, High, Low, Close) sempre del subset.

\begin{table}[h]
	\centering
	\renewcommand{\arraystretch}{1.2} % Spaziatura tra righe
	\setlength{\tabcolsep}{8pt} % Spaziatura tra colonne
	\small % Riduci leggermente la dimensione del testo per adattare meglio la tabella
	\begin{tabular}{lcccc} % Prima colonna a sinistra, altre centrate
		\toprule
		& \textbf{Open} & \textbf{High} & \textbf{Low} & \textbf{Close} \\
		\midrule
		\textbf{Open}  & 1.0000  & 0.4716  & 0.4890  & 0.3141  \\
		\textbf{High}  & 0.4716  & 1.0000  & 0.3353  & 0.5610  \\
		\textbf{Low}   & 0.4890  & 0.3353  & 1.0000  & 0.5142  \\
		\textbf{Close} & 0.3141  & 0.5610  & 0.5142  & 1.0000  \\
		\bottomrule
	\end{tabular}
	\caption{Matrice di correlazione tra Open, High, Low e Close}
	\label{tab:matrice_correlazione}
\end{table}

\section{Assunzioni generali}

Considerando i dati analizzati e i risultati ottenuti dalle elaborazioni, possiamo formulare le seguenti assunzioni per l’applicazione dei modelli di copula:

\begin{enumerate}
	\item \textbf{Uniformità marginale} \\
	Una delle assunzioni principali per applicare i modelli di copula è che le variabili marginali siano uniformi. Nel nostro caso, i rendimenti logaritmici calcolati per le diverse variabili (Open, High, Low, Close) sono stati trasformati in modo tale da poter essere considerati approssimativamente stazionari, ma non sono ancora stati resi uniformi. Per l’applicazione delle copule, sarà necessario trasformare i rendimenti normalizzati in una distribuzione uniforme sull’intervallo \([0,1]\), ad esempio usando la funzione di distribuzione cumulativa empirica. Questo passaggio garantisce che le dipendenze tra variabili siano modellate correttamente senza distorsioni derivanti dalle distribuzioni marginali.
	
	\item \textbf{Stazionarietà dei dati} \\
	Per applicare correttamente i modelli di copula, è necessario che le serie temporali siano stazionarie, ovvero che le proprietà statistiche come media e varianza siano costanti nel tempo. Abbiamo trasformato i prezzi in rendimenti logaritmici per ottenere una serie più stazionaria rispetto ai dati originali di prezzo. Tuttavia, è possibile che ci siano ancora componenti non stazionarie, come trend residui o stagionalità, che potrebbero influire sui risultati. La verifica e il trattamento di eventuali residui non stazionari sono fondamentali per garantire la validità dei modelli di copula.
	
	\item \textbf{Struttura di dipendenza} \\
	L’analisi dei rendimenti logaritmici ha mostrato una correlazione positiva tra le variabili, sebbene con valori differenti per ciascuna coppia (ad esempio, correlazione relativamente più alta tra High e Close, e più bassa tra Open e Close). Questa osservazione implica che esiste una struttura di dipendenza tra le variabili di prezzo, che va oltre la correlazione lineare. Le copule ci permetteranno di catturare meglio questa dipendenza, soprattutto nei casi in cui le correlazioni sono condizionate da situazioni estreme (code pesanti).
	
	\item \textbf{Dipendenze di coda} \\
	Osservando la distribuzione dei rendimenti logaritmici, è evidente che la distribuzione presenta code più pesanti rispetto a una normale distribuzione Gaussiana. Questo suggerisce una maggiore probabilità di eventi estremi (sia positivi che negativi), specialmente durante periodi di volatilità del mercato. Pertanto, è ragionevole assumere che le variabili presentino dipendenza di coda, rendendo modelli come la copula di Student-t o la copula di Clayton adatti per catturare le correlazioni nelle code inferiori, particolarmente durante i ribassi di mercato.
	
	\item \textbf{Non-normalità delle distribuzioni marginali} \\
	I rendimenti logaritmici non seguono una distribuzione normale; piuttosto, mostrano asimmetria e code più pesanti. Sebbene la copula Gaussiana possa essere utilizzata per una prima analisi, è preferibile utilizzare copule come la Student-t per gestire deviazioni dalla normalità, particolarmente utili per modellare le code e le correlazioni durante gli eventi estremi.
	
	\item \textbf{Asimmetria nelle dipendenze} \\
	La matrice di correlazione calcolata tra i rendimenti (Open, High, Low, Close) mostra differenze nei livelli di correlazione tra le diverse variabili. Questa variabilità nelle correlazioni suggerisce che alcuni modelli di copula, come la Clayton (per le code inferiori) o la Gumbel (per le code superiori), possano offrire una descrizione più accurata delle dipendenze rispetto a una semplice copula Gaussiana.
	
	\item \textbf{Condizioni di diversificazione} \\
	La copula di Frank potrebbe essere adatta per modellare le dipendenze tra le variabili che non mostrano comportamenti particolarmente forti nelle code (ovvero, dipendenze moderate e stabili). Tuttavia, i dati indicano la presenza di code pesanti, quindi questa copula potrebbe essere utilizzata solo come confronto con modelli che catturano meglio le dipendenze estreme.
\end{enumerate}

\subsection{Conclusione sulle assunzioni}

Uniformità e stazionarietà sono requisiti fondamentali per l’applicazione dei modelli di copula. I dati sono stati trasformati per soddisfare parzialmente queste assunzioni.

Dipendenze di coda e non-normalità suggeriscono l’uso di copule robuste come la Student-t o modelli asimmetrici come la Clayton o la Gumbel per catturare meglio le relazioni tra variabili durante condizioni di stress di mercato.

L'asimmetria delle correlazioni evidenziata dalla matrice di correlazione indica la necessità di copule che possano gestire differenti tipi di dipendenze nelle code.

Queste assunzioni ci permettono di scegliere il modello di copula più appropriato per analizzare le dipendenze tra i rendimenti del DAX e comprendere meglio il comportamento del mercato in diverse condizioni economiche.




\part{Stima dei Parametri delle
	Copule e Implementazione
	Pratica}

\chapter{Stime dei parametri}

\section{Introduzione alla Stima dei Parametri delle Copule}

La stima dei parametri delle copule è cruciale per descrivere con precisione il tipo e il grado di dipendenza tra variabili. In particolare, in finanza:

\begin{itemize}
	\item \textbf{Gestione del rischio}: La corretta modellazione della dipendenza è essenziale per calcolare indicatori come il Value-at-Risk (VaR) e il Conditional VaR (CVaR), nonché per valutare il rischio di portafoglio in condizioni estreme.
	\item \textbf{Ottimizzazione del portafoglio}: La conoscenza della dipendenza consente di costruire portafogli ottimizzati, tenendo conto delle correlazioni non lineari tra asset.
	\item \textbf{Eventi estremi}: La modellazione della \textit{tail dependence} (dipendenza nelle code) permette di catturare correttamente la probabilità di eventi congiunti estremi, come crisi finanziarie o fallimenti simultanei di istituzioni.
\end{itemize}

L'importanza della stima risiede nella capacità di rappresentare accuratamente la struttura di dipendenza osservata nei dati, migliorando l'affidabilità e la precisione dei modelli finanziari.

Per stimare i parametri delle copule, esistono diversi metodi, ciascuno con vantaggi e limitazioni:

\begin{enumerate}
	\item \textbf{Metodo della Massima Verosimiglianza (MLE - Maximum Likelihood Estimation)}: Massimizzando la funzione di verosimiglianza costruita sui dati, si ottengono degli stimatori per i vari parametri.
	\item \textbf{Metodo dei Momenti}: Confronta i momenti osservati con quelli teorici per stimare i parametri della copula.
	\item \textbf{Metodi Bayesiani}: Utilizzano distribuzioni a priori per stimare i parametri delle copule e aggiornano le stime con i dati osservati.
\end{enumerate}

La stima dei parametri delle copule rappresenta un passaggio fondamentale nella modellazione della dipendenza tra variabili finanziarie. La scelta del metodo più appropriato dipende dalla complessità del modello, dalla disponibilità di dati e dai vincoli computazionali. L’utilizzo corretto delle copule migliora significativamente l’accuratezza dei modelli finanziari, con importanti applicazioni nella gestione del rischio, nella costruzione di portafogli e nella previsione di eventi estremi.

Nelle prossime pagine mostreremo un’analisi dettagliata dei vari metodi di
stima (anche un implementazione Python) e un applicazione pratica utilizzando i dati del DAX.

\section{Metodi di stima dei parametri}

\subsection{Stima di Massima Verosimiglianza (MLE)}

Il Metodo della Massima Verosimiglianza (MLE) é una delle tecniche
più utilizzate per la stima dei parametri delle copule grazie alla sua efficienza e alla solidità teorica. Questo metodo si basa sul principio di determinare i parametri che massimizzano la probabilità (o verosimiglianza)
dei dati osservati sotto il modello scelto.\\

\textbf{Procedura:}
\begin{enumerate}
	\item Supponiamo di avere \( n \) osservazioni \( x_i = (x_{i1}, x_{i2}, ..., x_{id}) \), con \( i = 1, ..., n \), da una distribuzione congiunta \( F(x; \theta) \), dove \( \theta \) è il vettore dei parametri della copula.
	\item La funzione di verosimiglianza è costruita come:
	\[
	L(\theta; x_1, ..., x_n) = \prod_{i=1}^{n} f(x_i; \theta)
	\]
	dove \( f(x_i; \theta) \) è la densità congiunta.
	\item Usando il Teorema di Sklar, la densità congiunta può essere scritta come:
	\[
	f(x_i; \theta) = c(F_1(x_{i1}), ..., F_d(x_{id}); \theta) \prod_{j=1}^{d} f_j(x_{ij})
	\]
	dove \( f_j(x_{ij}) \) sono le densità marginali e \( c(\cdot; \theta) \) è la densità della copula.
	\item Se le distribuzioni marginali sono note, la funzione di verosimiglianza dipende solo dalla copula:
	\[
	L(\theta) = \prod_{i=1}^{n} c(F_1(x_{i1}), ..., F_d(x_{id}); \theta)
	\]
	\item La stima \( \hat{\theta} \) dei parametri si ottiene massimizzando il logaritmo della funzione di verosimiglianza:
	\[
	\hat{\theta} = \arg \max_{\theta} \sum_{i=1}^{n} \log c(F_1(x_{i1}), ..., F_d(x_{id}); \theta)
	\]
\end{enumerate}

Dopo aver spiegato come funziona il metodo matematicamente, forniamo dei motivi per cui l'MLE è comunemente utilizzato per le copule.

\subsubsection{Vantaggi del MLE:}

\begin{itemize}
	\item \textbf{Efficienza statistica:}\\
	L'MLE produce stime consistenti, non distorte e asintoticamente efficienti sotto ipotesi regolari, garantendo la massima accuratezza possibile per grandi campioni.
	
	\item \textbf{Flessibilità:}\\
	L'MLE è adatta sia a copule con marginali note che a copule con marginali sconosciute (in quest'ultimo caso, si utilizza la variante del \textit{Pseudo-MLE}).
	
	\item \textbf{Compatibilità con il teorema di Sklar:}\\
	Grazie alla separazione tra marginali e dipendenza, l'MLE consente di stimare parametri che riflettono esclusivamente la struttura di dipendenza, riducendo l'influenza delle marginali.
	
	\item \textbf{Applicazioni pratiche:}\\
	In ambito finanziario, l'MLE è particolarmente efficace per stimare copule complesse (es. copule \(t\) o Archimedee) che modellano fenomeni come la \textit{tail dependence} (dipendenza nelle code).
	
	\item \textbf{Supporto computazionale:}\\
	L'MLE è ben supportata da software statistici e librerie numeriche (ad esempio, Python, R, MATLAB), rendendola una scelta praticabile anche per problemi reali con dataset di grandi dimensioni.
\end{itemize}

Il metodo della Massima Verosimiglianza `e una tecnica robusta e versatile per la stima dei parametri delle copule, garantendo precisione
ed efficienza nella modellazione della dipendenza. La sua applicazione,
unita alla separazione tra marginali e struttura di dipendenza, ne fa
uno strumento indispensabile nella modellazione finanziaria e in molte
altre discipline quantitative.\\

Vediamo anche qualche limitazione di questo metodo.\\

\subsubsection{Limitazioni del MLE:}

\begin{itemize}
	\item \textbf{Complessità Computazionale:}
	\begin{itemize}
		\item La funzione di verosimiglianza può diventare complessa da calcolare, specialmente per modelli con molte variabili o copule con parametri complessi.
		\item Richiede spesso ottimizzazione numerica iterativa (es. Newton-Raphson, metodi stocastici), che può essere lenta o soggetta a problemi di convergenza.
	\end{itemize}
	
	\item \textbf{Sensibilità ai Dati:}
	\begin{itemize}
		\item L'MLE assume che il modello scelto sia corretto. In ambito finanziario, però, i dati reali possono violare le ipotesi standard (es. non normalità delle marginali, dati mancanti, errori di misurazione).
		\item Le stime possono risultare inefficaci se il modello non rappresenta bene i dati.
	\end{itemize}
	
	\item \textbf{Dipendenza dalle Condizioni Asintotiche:}
	\begin{itemize}
		\item L'efficienza dell'MLE è garantita solo per campioni sufficientemente grandi. Nei dataset finanziari con pochi dati (es. eventi estremi rari), l'MLE può essere instabile.
	\end{itemize}
	
	\item \textbf{Problemi di Robustezza:}
	\begin{itemize}
		\item L'MLE è sensibile agli \textit{outlier}, che sono comuni nei dati finanziari (ad esempio, picchi improvvisi di volatilità).
		\item Per mitigare questo problema, possono essere necessari metodi alternativi.
	\end{itemize}
	
	\item \textbf{Vincoli Numerici e Positività:}
	\begin{itemize}
		\item Nel caso di copule multivariate, come la copula Gaussiana o t-Student, la matrice di correlazione deve essere positiva definita. 
		\item L'imposizione di vincoli aggiunge complessità computazionale.
	\end{itemize}
\end{itemize}

\subsubsection{Considerazioni finali}

L'MLE è uno strumento potente e teoricamente solido per la stima dei parametri in contesti finanziari. Tuttavia, la sua applicazione pratica deve tenere conto delle caratteristiche specifiche dei dati finanziari e della complessità computazionale del modello scelto. In alternativa, quando i limiti dell'MLE diventano problematici, possono essere considerati altri approcci, come il metodo dei momenti, il metodo bayesiano o il metodo pseudo-MLE.\\

\subsection{Implementazione Python per una copula Student t:}
Mostriamo ora un'implementazione in Python di questo metodo.\\
Iniziamo mostrando le librerie necessarie per l'esecuzione dello script.
\begin{Verbatim}
	import numpy as np
	import pandas as pd
	import scipy.stats as stats
	import scipy.optimize as optimize
	import matplotlib.pyplot as plt
	from scipy.special import gamma
	import os
\end{Verbatim}
Successivamente sará necessario un dataset giá normalizzato su cui applicare questo metodo. In questo esempio viene utilizzato il dataset $uniform\_data$ mostrato nella terza parte.\\
In questo esempio utilizzeremo una copula Student t bivariata, le cui variabili saranno i valori $Open$ e i valori $Close$ che andremo ad indicare rispettivamente con $u$ e $v$.\\
\begin{Verbatim}
	# Selezione di due colonne per la copula
	u, v = uniform_data['Open'].values, uniform_data['Close'].values
\end{Verbatim}
Implementiamo direttamente la Copula Student-t ricordando che la sua densitá é:\\
\[
c(u,v; \rho, \nu) =
\frac{\Gamma\left(\frac{\nu+2}{2}\right) \Gamma\left(\frac{\nu}{2}\right)^{-1} \left(1 + \frac{x^2 + y^2 - 2\rho xy}{\nu(1 - \rho^2)}\right)^{-\frac{\nu+2}{2}}}
{\sqrt{1 - \rho^2} \cdot \Gamma\left(\frac{\nu+1}{2}\right)^2 \Gamma\left(\frac{\nu}{2}\right)^{-2} \left(1 + \frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}} \left(1 + \frac{y^2}{\nu}\right)^{-\frac{\nu+1}{2}}}
\]

dove \( x = t_{\nu}^{-1}(u) \) e \( y = t_{\nu}^{-1}(v) \).
\begin{Verbatim}
	def t_copula_pdf(u, v, rho, df):
	x = stats.t.ppf(u, df)
	y = stats.t.ppf(v, df)
	
	# Protezione contro overflow/underflow
	try:
	
	numerator = gamma((df + 2) / 2) * gamma(df / 2) * (
	1 + (x ** 2 + y ** 2 - 2 * rho * x * y) /
	 (df * (1 - rho ** 2))) ** (-(df + 2) / 2)
	 
	denominator = gamma((df + 1) / 2) ** 2 * df * np.pi *
	 np.sqrt(1 - rho ** 2) * (1 + x ** 2 / df) ** (
	-(df + 1) / 2) * (1 + y ** 2 / df) ** (-(df + 1) / 2)
	
	result = numerator / denominator
	
	# Sostituire infiniti o NaN con un valore molto piccolo ma positivo
	result = np.where(np.isfinite(result) & (result > 0), result, 1e-10)
	return result
	except:
	# In caso di errore, ritorna un valore di default
	return np.ones_like(u) * 1e-10
\end{Verbatim}
Implementiamo la funzione likelihood
\begin{verbatim}
	def t_copula_likelihood(params, u, v):
	rho = np.tanh(params[0])
	df = np.exp(params[1]) + 2
	likelihoods = t_copula_pdf(u, v, rho, df)
	
	# Gestione di valori non validi
	valid_idx = ~np.isnan(likelihoods) & (likelihoods > 0)
	if not np.any(valid_idx):
	return 1e10
	return -np.sum(np.log(likelihoods[valid_idx]))
\end{verbatim}
Implementiamo la stima MLE
\begin{verbatim}
	def estimate_t_copula_MLE(u, v):
	initial_guess = [0.5, np.log(8)]
	bounds = [(-10, 10), (-10, 10)]  # Limiti per rho e log(df)
	result = optimize.minimize(t_copula_likelihood, initial_guess, args=(u, v),
	method='L-BFGS-B', bounds=bounds)
	rho_estimated = np.tanh(result.x[0])
	df_estimated = np.exp(result.x[1]) + 2
	return rho_estimated, df_estimated
\end{verbatim}
Infine possiamo osservare i risultati ottenuti in questo modo
\begin{verbatim}
	rho_t_mle, df_t_mle = estimate_t_copula_MLE(u, v)
	print(f" Parametro stimato (rho) per la Copula t-Student: {rho_t_mle:.4f}")
	print(f" Gradi di libertà stimati per la Copula t-Student: {df_t_mle:.2f}")
\end{verbatim}
I risultati numerici e i valori di stima del nostro dataset saranno calcolati e mostrati più avanti

\subsection{Metodo dei Momenti}

Il metodo dei momenti è una tecnica di stima che si basa sull’equiparazione dei momenti teorici di una distribuzione con i momenti campionari calcolati dai dati osservati. Nel contesto delle copule, il metodo può essere utilizzato per stimare i parametri che descrivono la dipendenza tra le variabili aleatorie, garantendo coerenza tra la struttura di dipendenza modellata e quella osservata.

\subsubsection{Applicazione alle copule}

Le copule sono funzioni che legano le distribuzioni marginali di variabili aleatorie alla loro distribuzione congiunta, separando la dipendenza dalla struttura marginale. Per stimare i parametri di una copula con il metodo dei momenti:

\begin{enumerate}
	\item \textbf{Scelta dei momenti di interesse}: Si identificano statistiche che catturano la dipendenza (ad esempio, Kendall’s \( \tau \), Spearman’s \( \rho \) o altre misure di concordanza).
	\item \textbf{Calcolo dei momenti campionari}: Si calcolano i momenti empirici dai dati osservati, ad esempio, calcolando \( \tau \) o \( \rho \) sui campioni.
	\item \textbf{Imposizione di uguaglianza}: I momenti teorici della copula, funzione dei parametri da stimare, vengono eguagliati ai momenti campionari.
	\item \textbf{Risoluzione del sistema}: Si risolvono le equazioni risultanti per determinare i parametri della copula.
\end{enumerate}

\subsubsection{Ambiti di Applicazione}

Il metodo dei momenti applicato alle copule trova largo impiego in settori dove è cruciale modellare la dipendenza tra variabili, come:
\begin{itemize}
	\item \textbf{Finanza}: Per analizzare la dipendenza tra rendimenti di asset.
	\item \textbf{Assicurazioni e gestione del rischio}: Per modellare eventi estremi correlati, come sinistri catastrofici.
\end{itemize}

\subsubsection{Vantaggi e Svantaggi}

Elenchiamo alcuni pro e contro di utilizzare questo metodo:

\paragraph{Vantaggi}
\begin{itemize}
	\item \textbf{Semplicità computazionale}: Rispetto ad altri metodi (ad esempio, la massima verosimiglianza), il metodo dei momenti è spesso più semplice da implementare e richiede meno assunzioni sul modello.
	\item \textbf{Intuitività}: L’approccio è facilmente interpretabile grazie al legame diretto con misure di dipendenza osservabili come \( \tau \) e \( \rho \).
	\item \textbf{Robustezza}: È meno sensibile a errori nelle specifiche delle distribuzioni marginali.
\end{itemize}

\paragraph{Svantaggi}
\begin{itemize}
	\item \textbf{Perdita di efficienza}: Gli stimatori ottenuti non sono sempre efficienti, il che significa che potrebbero avere una varianza maggiore rispetto a quelli della massima verosimiglianza.
	\item \textbf{Dipendenza dalla scelta dei momenti}: La qualità della stima dipende fortemente dalla scelta dei momenti, che potrebbero non catturare adeguatamente la complessità della dipendenza.
	\item \textbf{Limitazioni con dati scarsi}: Con campioni piccoli, i momenti campionari potrebbero essere poco rappresentativi.
\end{itemize}

\subsubsection{Limiti in contesti multivariati}

Un altro fattore da tenere in considerazione è il numero di variabili a cui è applicato, in quanto potrebbe avere delle limitazioni come:

\begin{enumerate}
	\item \textbf{Difficoltà di Generalizzazione in Dimensioni Elevate}: 
	\begin{itemize}
		\item Le misure di concordanza (ad esempio, Kendall’s \( \tau \)) diventano difficili da calcolare per coppie di variabili.
		\item Il numero di parametri della copula aumenta esponenzialmente con la dimensione, rendendo il sistema di equazioni derivato dai momenti difficile da risolvere.
	\end{itemize}
	
	\item \textbf{Sensibilità ai Dati Empirici}: I momenti campionari utilizzati per la stima sono sensibili alla presenza di outlier o dati scarsi, introducendo bias significativi nella stima dei parametri.
	
	\item \textbf{Rappresentazione Limitata della Dipendenza}:
	\begin{itemize}
		\item Il metodo dei momenti si basa su misure sintetiche della dipendenza (come \( \tau \) o \( \rho \)), che potrebbero non cogliere adeguatamente relazioni complesse, come:
		\begin{itemize}
			\item Dipendenze non lineari.
			\item Code pesanti o asimmetrie estreme.
		\end{itemize}
	\end{itemize}
	
	\item \textbf{Convergenza Non Ottimale}: Gli stimatori ottenuti con il metodo dei momenti non sono necessariamente efficienti in termini di varianza, soprattutto quando i dati presentano una struttura di dipendenza complessa.
\end{enumerate}

\newpage

\subsection{Il Tau di Kendall}

Abbiamo parlato più volte della misura \( \tau \) di Kendall, spieghiamo meglio in cosa consiste.

Il tau di Kendall è una misura di concordanza per due variabili casuali continue. È definito come la differenza tra la probabilità di concordanza e la probabilità di discordanza. Può essere interpretato come la probabilità che due coppie di osservazioni scelte a caso dal campione siano concordanti meno la probabilità che siano discordanti.

\subsubsection{Principio di base}

Dato un insieme di \( n \) coppie di dati \( (x_1, y_1), (x_2, y_2), \dots, (x_n, y_n) \), il Kendall’s Tau si basa sul confronto tra tutte le coppie di osservazioni:

\begin{enumerate}
	\item \textbf{Coppia concordante}: Una coppia \( (x_i, y_i) \) e \( (x_j, y_j) \) è \textit{concordante} se i segni delle differenze \( (x_j - x_i) \) e \( (y_j - y_i) \) sono uguali (entrambi positivi o entrambi negativi). In parole semplici:
	\[
	(x_j - x_i)(y_j - y_i) > 0
	\]
	
	\item \textbf{Coppia discordante}: Una coppia \( (x_i, y_i) \) e \( (x_j, y_j) \) è \textit{discordante} se i segni delle differenze \( (x_j - x_i) \) e \( (y_j - y_i) \) sono opposti (uno positivo e l’altro negativo). In parole semplici:
	\[
	(x_j - x_i)(y_j - y_i) < 0
	\]
	
	\item \textbf{Coppia legata}: Una coppia è \textit{legata} se \( x_j = x_i \) o \( y_j = y_i \).
\end{enumerate}

\subsubsection{Formula del Kendall’s Tau}

Il Kendall’s Tau è calcolato come:

\[
\tau = \frac{C - D}{\binom{n}{2}}
\]

Dove:
\begin{itemize}
	\item \( C \) è il numero di coppie concordanti.
	\item \( D \) è il numero di coppie discordanti.
	\item \( \binom{n}{2} = \frac{n!}{2(n-2)!} = \frac{n(n-1)}{2} \) è il numero totale di coppie.
\end{itemize}

\paragraph{Valori di \( \tau \):}
\begin{itemize}
	\item \( \tau = 1 \): perfetta concordanza.
	\item \( \tau = -1 \): perfetta discordanza.
	\item \( \tau = 0 \): assenza di relazione (casuale).
\end{itemize}

\subsubsection{Differenza con il Coefficiente di Correlazione di Spearman}

Sebbene sia il tau di Kendall che il rho di Spearman misurino la concordanza, i loro valori possono essere diversi. La relazione tra i due dipende dalla particolare famiglia di copule che descrive la dipendenza tra le variabili. Ci sono disuguaglianze universali che mettono in relazione i due coefficienti, ma la relazione specifica può variare.

La scelta tra il tau di Kendall e il rho di Spearman dipende spesso dalla specifica applicazione e dalle preferenze personali. Il tau di Kendall è talvolta preferito per la sua interpretazione probabilistica più diretta e per la sua robustezza agli outlier. Il rho di Spearman è talvolta preferito per la sua relazione con la correlazione lineare e la sua maggiore sensibilità alle differenze nei ranghi.

In generale, sia il tau di Kendall che il rho di Spearman sono misure utili della concordanza e possono fornire informazioni preziose sulla dipendenza tra due variabili casuali.

\subsection{Relazioni tra parametri delle copule e misure di dipendenza}

Le formule che legano il tau di Kendall ai parametri delle diverse copule:\\

\begin{enumerate}
	\item \textbf{Copula Gaussiana}
	\[
	\tau = \frac{2}{\pi} \arcsin(\rho)
	\]
	\textbf{Inversione:}
	\[
	\rho = \sin\left(\frac{\pi \tau}{2}\right)
	\]
	\item \textbf{Copula t-Student}
	\[
	\tau = \frac{2}{\pi} \arcsin(\rho)
	\]
	Stessa relazione della Gaussiana, indipendente dai gradi di libertà \( \nu \).
	\item \textbf{Copula di Clayton}
	\[
	\tau = \frac{\theta}{\theta + 2}
	\]
	\textbf{Inversione:}
	\[
	\theta = \frac{2\tau}{1 - \tau}
	\]
	\item \textbf{Copula di Gumbel}
	\[
	\tau = 1 - \frac{1}{\theta}
	\]
	\textbf{Inversione:}
	\[
	\theta = \frac{1}{1 - \tau}
	\]
	\item \textbf{Copula di Frank}
	\[
	\tau = 1 - \frac{4}{\theta} \left( 1 - D_1(\theta) \right)
	\]
	dove \( D_1(\theta) \) è la funzione di Debye:
	\[
	D_1(\theta) = \frac{1}{\theta} \int_0^{\theta} \frac{t}{e^t - 1} dt
	\]
	Non esiste una formula di inversione semplice.
\end{enumerate}

\subsection{Codice Python metodo dei momenti}

\subsubsection{Applicazione alla copula Gumbel}

Per prima cosa definiamo la relazione tra il tau di Kendall e il parametro theta della Gumbel.

\begin{verbatim}
	def gumbel_theta_kendall(tau):
	"""
	Calcola il parametro theta della copula di Gumbel a partire da tau di Kendall.
	Formula: theta = 1 / (1 - tau)
	"""
	return 1 / (1 - tau)
\end{verbatim}

Successivamente impostiamo una funzione che calcoli il tau di Kendall in maniera ottimizzata (non possiamo calcolarlo come nell'esercizio numerico in seguito all'elevato costo computazionale).

\begin{Verbatim}
	from scipy.stats import kendalltau
	
	def kendall_tau_multivariate_optimized(dataset):
	"""
	Calcola il Kendall tau medio per un dataset multivariato usando scipy.
	"""
	n_vars = dataset.shape[1]
	taus = []
	
	# Calcola Kendall tau per ogni coppia di variabili
	for i in range(n_vars):
	for j in range(i + 1, n_vars):
	tau, _ = kendalltau(dataset[:, i], dataset[:, j])
	taus.append(tau)
	
	# Restituisce il tau medio e il numero di coppie concordanti/discordanti
	tau_mean = np.mean(taus)
	return tau_mean
\end{Verbatim}

Per concludere implementiamo il metodo dei momenti ottimizzato.

\begin{Verbatim}
	def method_of_moments_gumbel_optimized(dataset):
	"""
	Applica il metodo dei momenti per stimare
	il parametro theta della copula di Gumbel.
	"""
	# Calcola il tau di Kendall medio
	tau_mean = kendall_tau_multivariate_optimized(dataset)
	
	# Calcola il parametro theta
	theta = gumbel_theta_kendall(tau_mean)
	return theta, tau_mean
\end{Verbatim}

Ora non ci resta che applicare questo metodo al nostro dataset $uniform \_data$ e ottenere il risultato.

\begin{Verbatim}
	# Stima il parametro theta
	theta_gumbel, tau_mean = method_of_moments_gumbel_optimized(uniform_data)
	print(f"Parametro theta stimato della Gumbel: {theta_gumbel:.4f}")
\end{Verbatim}


\newpage
\subsubsection{Applicazione alla copula Clayton}

Definiamo la relazione tra il tau di Kendall e il parametro theta della Clayton.

\begin{verbatim}
	def clayton_theta_kendall(tau):
	"""
	Calcola il parametro theta della copula di Clayton a partire da tau di Kendall.
	Formula: theta = 2*tau / (1 - tau)
	"""
	return 2*tau / (1 - tau)
\end{verbatim}

La funzione per il calcolo del tau di Kendal é esattamente uguale alla precedente.\\

Implementiamo il metodo dei momenti ottimizzato.

\begin{Verbatim}
	def method_of_moments_clayton_optimized(dataset):
	"""
	Applica il metodo dei momenti per stimare il parametro theta della copula di Clayton.
	"""
	# Calcola il tau di Kendall medio
	tau_mean = kendall_tau_multivariate_optimized(dataset)
	
	# Calcola il parametro theta
	theta = clayton_theta_kendall(tau_mean)
	return theta, tau_mean
\end{Verbatim}

Applichiamo questo metodo al nostro dataset $uniform \_data$ e otteniamo il risultato.

\begin{Verbatim}
	# Stima il parametro theta
	theta_clayton, tau_mean = method_of_moments_clayton_optimized(uniform_data)
	print(f"Parametro theta stimato della Clayton: {theta_clayton:.4f}")
\end{Verbatim}


\newpage
\subsection{Stima Bayesiana}

La stima bayesiana è un approccio alla statistica basato sul teorema di Bayes, che combina informazioni a priori con dati osservati per aggiornare la nostra conoscenza su un fenomeno incerto. È ampiamente utilizzata in applicazioni come l’apprendimento automatico, l’inferenza statistica e il processo decisionale.

\subsubsection{Teorema di Bayes}

La statistica bayesiana si fonda sul teorema di Bayes, che dice che, data due variabili aleatorie (anche vettoriali) \( X \) e \( Y \), allora:

\[
f(x \mid y) = \frac{f(y, x)}{f(y)} = \frac{f(y \mid x) f(x)}{f(y)}
\]

dove \( f(y) = \int f(y, x)dx \). \\

Il teorema di Bayes permette di passare dalla condizionata di \( y \) rispetto a \( x \) a quella di \( x \) rispetto a \( y \).\\

Un altro modo di vedere il teorema di Bayes è di tipo “iterativo”:
\begin{itemize}
	\item Ho una distribuzione a priori su \( x \), \( f(x) \).
	\item Osservo una nuova variabile \( y \), che dipende da \( x \), tramite \( f(y \mid x) \).
	\item Allora l’informazione che ho su \( x \), dopo aver osservato \( y \), cambia in \( f(x \mid y) \).
\end{itemize}

Consideriamo ora \( x = \theta \) come parametri di una densità di probabilità (ad esempio \( \mu, \sigma^2 \) se parliamo di una normale), allora avremo:

\begin{itemize}
	\item \( f(\theta \mid y) \) è chiamata distribuzione a posteriori di \( \theta \).
	\item \( f(y \mid \theta) \) è la congiunta delle osservazioni, che è possibile vedere anche come la verosimiglianza.
	\item \( f(\theta) \) è la distribuzione a priori. Questa distribuzione riflette ciò che sappiamo dei parametri prima di osservare il campione \( y \).
	\item \( f(y) \) è la costante di normalizzazione, in genere meno rilevante poiché non dipende da \( \theta \).
\end{itemize}

La stima bayesiana, specialmente in combinazione con le copule, offre numerosi vantaggi in contesti caratterizzati da dati limitati o mercati volatili. Questi vantaggi emergono dal fatto che il paradigma bayesiano consente di integrare informazioni a priori, aggiornare le credenze in modo dinamico e modellare con precisione dipendenze complesse.

\subsubsection{Vantaggi Generali della Stima Bayesiana}

\begin{enumerate}
	\item \textbf{Integrazione di conoscenze a priori} \\
	La stima bayesiana consente di utilizzare informazioni preesistenti, come storie storiche o conoscenze di esperti, per costruire una distribuzione a priori. Questo è particolarmente utile in scenari con dati limitati.
	
	\item \textbf{Aggiornamento dinamico delle credenze} \\
	Il paradigma bayesiano permette di aggiornare i parametri di interesse man mano che nuovi dati diventano disponibili, utilizzando il teorema di Bayes.
	
	\item \textbf{Distribuzione a posteriori invece di stime puntuali} \\
	La stima bayesiana produce distribuzioni a posteriori che rappresentano l’incertezza sui parametri, offrendo un quadro probabilistico più robusto rispetto all’uso di stime puntuali.
	
	\item \textbf{Gestione di modelli complessi} \\
	Grazie alla capacità di combinare la struttura di dipendenza (copule) con modelli marginali, la stima bayesiana è particolarmente adatta per problemi multivariati o non lineari.
\end{enumerate}

\subsubsection{Vantaggi Specifici nei Contesti con Copule}

Le \textbf{copule} sono utili per modellare le dipendenze tra variabili aleatorie, anche quando queste non seguono distribuzioni gaussiane o hanno comportamenti estremi (ad esempio, correlazioni nelle code). Quando abbinate alla stima bayesiana, i vantaggi si amplificano nei seguenti modi:

\begin{enumerate}
	\item \textbf{Modellazione della dipendenza in contesti di dati limitati} \\
	Nei mercati finanziari o assicurativi con dati scarsi, l’approccio bayesiano consente di sfruttare distribuzioni a priori sui parametri della copula, riducendo il rischio di sottostimare o sovrastimare la dipendenza.
	
	\item \textbf{Robustezza in mercati volatili} \\
	Le copule consentono di modellare cambiamenti nella dipendenza tra variabili in mercati turbolenti. L’approccio bayesiano aggiorna dinamicamente questi parametri con nuovi dati, migliorando la risposta ai cambiamenti rapidi.
	
	\item \textbf{Modellazione delle code} \\
	Le copule come la Gumbel o la t-Student sono in grado di catturare la dipendenza nelle code della distribuzione. In combinazione con l’approccio bayesiano, è possibile stimare parametri di coda con maggiore affidabilità anche in presenza di pochi dati osservati.
	
	\item \textbf{Riduzione del rischio di overfitting} \\
	La stima bayesiana utilizza distribuzioni a priori regolarizzanti, che prevengono l’overfitting tipico nei modelli di dipendenza con molti parametri. Questo é particolarmente importante in contesti
	con dati limitati.
\end{enumerate}

\subsubsection{Applicazioni Specifiche}

\begin{enumerate}
	\item \textbf{Finanza (Portafogli e Rischio)}
	\begin{itemize}
		\item La stima bayesiana con copule permette di modellare la dipendenza tra rendimenti degli asset in portafoglio, considerando eventi estremi (correlazioni nelle code).
		\item È utile per stimare la \textit{Value at Risk} (VaR) e il \textit{Conditional VaR} (CVaR) in mercati volatili.
	\end{itemize}
	
	\item \textbf{Assicurazioni (Rischi Multivariati)}
	\begin{itemize}
		\item Modellare la dipendenza tra sinistri assicurativi (ad esempio, danni da eventi naturali correlati).
		\item L’approccio bayesiano consente di incorporare conoscenze a priori su rischi rari ma severi.
	\end{itemize}
	
	\item \textbf{Analisi Multivariata e Dati Mancanti}
	\begin{itemize}
		\item Quando alcune variabili o dati sono mancanti, le copule e la stima bayesiana consentono di stimare la dipendenza tra variabili osservate e non osservate.
	\end{itemize}
\end{enumerate}

\subsection{Metodo MCMC in Python}
Il metodo di Monte Carlo a catena di Markov (MCMC) è una tecnica di campionamento utilizzata per approssimare distribuzioni di probabilità complesse. Questo metodo si basa sulla costruzione di una catena di Markov il cui stato stazionario coincide con la distribuzione target desiderata. Tra gli algoritmi più comuni per l'MCMC troviamo Metropolis-Hastings e Gibbs Sampling.

L'MCMC è ampiamente usato in statistica bayesiana, econometria, machine learning e fisica computazionale per la stima di parametri in modelli complessi dove l'integrazione diretta non è possibile.

\subsubsection{Implementazione in Python}
Di seguito viene presentata un'implementazione del metodo MCMC in Python utilizzando la copula gaussiana per modellare la dipendenza tra variabili finanziarie.\\

Librerie utilizzate
\begin{verbatim}
	import numpy as np
	import pandas as pd
	import scipy.stats as stats
	import matplotlib.pyplot as plt
	from scipy.optimize import minimize
\end{verbatim}
Utilizziamo sempre le due colonne $Open$ e $Close$ del dataset $uniform\_data$ come campioni chiamndole corrispettivamente $u\_unif$ e $v\_unif$.\\

Definizione della funzione di verosimiglianza per la Copula Gaussiana
\begin{verbatim}
	def gaussian_copula_log_likelihood(rho, u, v):
	"""Calcola la log-verosimiglianza della copula gaussiana."""
	if abs(rho) >= 0.999:  # Evita problemi numerici ai bordi
	return -np.inf
	
	x = stats.norm.ppf(u)
	y = stats.norm.ppf(v)
	
	# Log-verosimiglianza della copula gaussiana
	term1 = -0.5 * np.log(1 - rho ** 2)
	term2 = -(rho ** 2 * (x ** 2 + y ** 2) - 2 * rho * x * y) / (2 * (1 - rho ** 2))
	
	return np.sum(term1 + term2)
\end{verbatim}
Ora definiamo l'algoritmo MCMC migliorato con multiple chain e diagnostica robusta
\begin{verbatim}
	def improved_metropolis_hastings(log_likelihood, u, v, n_chains=4, 
	n_iter=20000, burn_in=5000,
	target_acceptance=0.234):
	"""
	Implementazione migliorata di Metropolis-Hastings con catene multiple
	e adattamento del passo basato su Gelman et al. (1996).
	"""
	n_params = 1  # Dimensione del parametro (solo rho)
	
	# Inizializzazione di catene multiple con punti di partenza diversi
	chains = np.zeros((n_chains, n_iter, n_params))
	
	# Punti di partenza distribuiti uniformemente tra -0.9 e 0.9
	starting_points = np.linspace(-0.8, 0.8, n_chains)
	
	# Parametri per l'adattamento
	gamma1 = 0.75  # Parametro che controlla l'adattamento iniziale
	adaptation_steps = int(n_iter * 0.6)  # Numero di passi di adattamento
	
	# Memorizza i tassi di accettazione e le larghezze delle proposte
	acceptance_rates = np.zeros(n_chains)
	proposal_widths = np.ones(n_chains) * 0.1  # Valori iniziali
	
	print("\n====== AVVIO MCMC CON MULTIPLE CATENE ======")
	
	for c in range(n_chains):
	chain = chains[c]
	chain[0, 0] = starting_points[c]  # Punto di partenza
	accepted = 0
	
	print(f"- Catena {c + 1}: punto di partenza = {starting_points[c]:.4f}")
	
	for i in range(1, n_iter):
	current_rho = chain[i - 1, 0]
	
	# Passo adattivo che diminuisce con le iterazioni
	if i <= adaptation_steps:
	adapt_factor = (i / adaptation_steps) ** gamma1
	current_width = proposal_widths[c] * 
	(1 - adapt_factor) + 0.1 * adapt_factor
	else:
	current_width = proposal_widths[c]
	
	# Proposta: U(-$\delta$,$\delta$) centrata sull'attuale valore
	delta = current_width
	proposal = current_rho + np.random.uniform(-delta, delta)
	
	# Rifletti se fuori dai limiti (-0.999, 0.999)
	if proposal <= -0.999:
	proposal = -0.999 + ((-0.999) - proposal)
	elif proposal >= 0.999:
	proposal = 0.999 - (proposal - 0.999)
	
	# Calcola il rapporto di accettazione
	log_p_current = log_likelihood(current_rho, u, v)
	log_p_proposal = log_likelihood(proposal, u, v)
	log_accept_ratio = log_p_proposal - log_p_current
	
	# Accetta o rifiuta
	if np.log(np.random.random()) < log_accept_ratio:
	chain[i, 0] = proposal
	accepted += 1
	else:
	chain[i, 0] = current_rho
	
	# Adatta la larghezza della proposta
	if i % 500 == 0 and i <= adaptation_steps:
	batch_acceptance = accepted / i
	
	# Aggiusta la larghezza per avvicinarsi al tasso di accettazione target
	if batch_acceptance > target_acceptance:
	proposal_widths[c] *= 1.1  # Aumenta
	else:
	proposal_widths[c] *= 0.9  # Diminuisci
	
	# Limita la larghezza per sicurezza
	proposal_widths[c] = max(0.01, min(1.0, proposal_widths[c]))
	
	if i % 5000 == 0:
	print(f"  Iterazione {i}, catena {c + 1}: accettazione = {batch_acceptance:.4f}, "
	f"larghezza = {proposal_widths[c]:.4f}")
	
	# Statistiche finali per questa catena
	acceptance_rates[c] = accepted / n_iter
	print(f"- Catena {c + 1} completata: accettazione = {acceptance_rates[c]:.4f}, "
	f"larghezza finale = {proposal_widths[c]:.4f}")
	
	# Rimuovi burn-in e unisci le catene per l'analisi
	combined_samples = chains[:, burn_in:, 0].flatten()
	
	return chains, acceptance_rates, combined_samples
\end{verbatim}
Ora eseguiamo MCMC migliorato
\begin{verbatim}
	n_chains = 4
	n_iter = 20000
	burn_in = 5000
	
	chains, acceptance_rates, combined_samples = improved_metropolis_hastings(
	gaussian_copula_log_likelihood,
	u_unif,
	v_unif,
	n_chains=n_chains,
	n_iter=n_iter,
	burn_in=burn_in
	)
\end{verbatim}
Verifichiamo la convergenza del metodo tramite la diagnostica di Gelman-Rubin.\\

La diagnostica di Gelman-Rubin, nota anche come \(\hat{R}\), è un metodo per valutare la convergenza delle catene MCMC. Confronta la varianza tra le catene con la varianza interna delle singole catene, fornendo un'indicazione di stabilità del campionamento. Se \(\hat{R} \approx 1\), le catene sono considerate convergenti; valori superiori a 1.1 indicano potenziali problemi di convergenza e la necessità di iterazioni aggiuntive.

\begin{verbatim}
	def gelman_rubin(chains, burn_in=0):
	"""Calcola la diagnostica R-hat di Gelman-Rubin."""
	n_chains, n_iter, n_params = chains.shape
	
	if burn_in > 0:
	chains = chains[:, burn_in:, :]
	
	n_iter = chains.shape[1]
	
	# Medie delle catene
	chain_means = np.mean(chains, axis=1)  # shape: (n_chains, n_params)
	
	# Varianza tra le catene
	B = n_iter * np.var(chain_means, axis=0, ddof=1)  # shape: (n_params,)
	
	# Varianza entro le catene
	W = np.mean(np.var(chains, axis=1, ddof=1), axis=0)  # shape: (n_params,)
	
	# Stima complessiva della varianza
	var_hat = ((n_iter - 1) / n_iter) * W + (1 / n_iter) * B
	
	# Calcolo R-hat
	R_hat = np.sqrt(var_hat / W)
	
	return R_hat
\end{verbatim}
Infine visualizziamo i risultati.
\begin{verbatim}
	rho_mean = np.mean(combined_samples)
	rho_std = np.std(combined_samples)
	rho_median = np.median(combined_samples)
	rho_ci = np.percentile(combined_samples, [2.5, 97.5])
	
	print("\n====== RISULTATI FINALI ======")
	print(f" Stima Bayesiana di $\rho$ con MCMC: {rho_mean:.4f} ± {rho_std:.4f}")
	print(f" Mediana: {rho_median:.4f}")
	print(f" Intervallo di credibilità al 95%: [{rho_ci[0]:.4f}, {rho_ci[1]:.4f}]")
	print(f" Dimensione del campione efficace: {len(combined_samples)}")
	
\end{verbatim}
\newpage
\section{Implementazione Pratica: Applicazione ai Dati del DAX}
\subsection{Preparazione dei dati}
La descrizione dettagliata del dataset utilizzato è già fornita nella terza parte della tesi. In questa sezione, ci concentreremo invece sull'implementazione di un processo strutturato per la preparazione e la verifica dei dati, con l'obiettivo di garantirne la qualità e l'affidabilità per le analisi successive.

Il codice sviluppato esegue una serie di operazioni di diagnostica e pulizia sui dati di mercato, consentendo di ottenere un dataset coerente, privo di errori e pronto per le operazioni analitiche.\\

Librerie utilizzate
\begin{verbatim}
	import pandas as pd
	import numpy as np
	import matplotlib.pyplot as plt
	import os
	import csv
\end{verbatim}

\subsubsection{Parte 1: Diagnostica del file CSV}

La prima fase del processo di preparazione dei dati consiste nella \textbf{diagnostica del file CSV}. 
L'obiettivo principale di questa sezione è garantire che il file di dati esista, sia leggibile e abbia una struttura coerente. 
Per farlo, vengono eseguite le seguenti operazioni:

\begin{enumerate}
	\item \textbf{Verifica dell'esistenza del file}
	\begin{itemize}
		\item Il codice controlla se il file specificato (\texttt{DAX\_3Y-1M.csv}) è presente nella directory corrente.
		\item Se il file non esiste, viene stampato un messaggio di errore con l'indicazione della directory corrente e dei file disponibili, facilitando il debugging.
	\end{itemize}
	
	\item \textbf{Ispezione delle prime righe del file}
	\begin{itemize}
		\item Se il file è stato trovato, vengono lette e stampate le prime dieci righe del file per ottenere una visione preliminare della sua struttura.
		\item Questo aiuta a identificare eventuali problemi evidenti, come caratteri strani, righe mancanti o errori di formattazione.
	\end{itemize}
	
	\item \textbf{Identificazione del delimitatore}
	\begin{itemize}
		\item Poiché i file CSV possono essere separati da virgole (\texttt{,}), punti e virgola (\texttt{;}), tabulazioni (\texttt{\textbackslash t}) o altri caratteri, viene utilizzata la funzione \texttt{csv.Sniffer()} per rilevare automaticamente il delimitatore corretto.
		\item Questo passaggio è essenziale per evitare errori di lettura quando si carica il file con \texttt{pandas}.
	\end{itemize}
	
	\item \textbf{Verifica dell'intestazione}
	\begin{itemize}
		\item Il codice verifica se il file contiene un'intestazione (ovvero nomi di colonne) e la conferma all'utente.
		\item Se l'intestazione non è presente, sarà necessario gestire i nomi delle colonne in modo esplicito nelle fasi successive.
	\end{itemize}
\end{enumerate}

Questa prima parte assicura che il file sia disponibile e che la sua struttura sia chiara prima di procedere con la lettura e la pulizia dei dati. 
Se il file presenta problemi, gli errori vengono segnalati tempestivamente, evitando che il codice fallisca nelle fasi successive.
Nel terminale visualizzo:\\

 \section*{DIAGNOSTICA DEL FILE CSV}
 
 \noindent\rule{\textwidth}{0.4pt}
 
 \textcolor{green}{File 'DAX\_3Y-1M.csv' trovato.}
 
 \noindent\rule{\textwidth}{0.4pt}
 
 \subsection*{Ispezione delle prime righe del file:}
 \begin{verbatim}
 	Riga 1: DateTime, Open, High, Low, Close
 	Riga 2: 02/01/2020 01:15:00 +01:00, 13174, 13194.5, 13171.5, 13177.5
 	Riga 3: 02/01/2020 01:16:00 +01:00, 13180, 13180.5, 13179, 13181.5
 	Riga 4: 02/01/2020 01:17:00 +01:00, 13182, 13182, 13180.5, 13181.5
 	...
 \end{verbatim}
 
 \textcolor{blue}{Delimitatore rilevato: ','}  
 
 \textcolor{blue}{Intestazione rilevata: False}  
 
 \subsection*{Lettura riuscita con delimitatore ','}
 \begin{verbatim}
 	DateTime                Open     High     Low     Close  Unnamed: 5
 	0  02/01/2020 01:15:00 +01:00  13174  13194.5  13171.5  13177.5      NaN
 	1  02/01/2020 01:16:00 +01:00  13180  13180.5  13179    13181.5      NaN
 	2  02/01/2020 01:17:00 +01:00  13182  13182    13180.5  13181.5      NaN
 	...
 \end{verbatim}
 
 \textcolor{green}{Lettura base riuscita}  
 
 \textbf{Dimensioni:} (616397, 6)  
 
 \textbf{Colonne:} ['DateTime', 'Open', 'High', 'Low', 'Close', 'Unnamed: 5']  
 
 \textbf{Prime 5 righe:}
 \begin{verbatim}
 	DateTime                Open     High     Low     Close  Unnamed: 5
 	0  02/01/2020 01:15:00 +01:00  13174  13194.5  13171.5  13177.5      NaN
 	1  02/01/2020 01:16:00 +01:00  13180  13180.5  13179    13181.5      NaN
 	2  02/01/2020 01:17:00 +01:00  13182  13182    13180.5  13181.5      NaN
 	...
 \end{verbatim}
 
\subsubsection{Parte 2: Lettura e Correzione dei Dati}

Dopo aver verificato l'esistenza e la struttura del file CSV nella fase di diagnostica, questa seconda parte si concentra sulla \textbf{lettura e correzione dei dati}, assicurandosi che il file venga caricato correttamente e che le colonne siano interpretate nel formato corretto.

Le operazioni principali svolte in questa fase sono:

\begin{enumerate}
	\item \textbf{Tentativi di lettura del file con diversi delimitatori}
	\begin{itemize}
		\item I file CSV possono essere separati da diversi caratteri (virgole \texttt{,}, punti e virgola \texttt{;}, tabulazioni \texttt{\textbackslash t}, barre verticali \texttt{|}).
		\item Il codice prova a leggere il file con ciascun delimitatore e controlla quale funziona senza errori.
		\item Se la lettura con un certo delimitatore ha successo, il codice interrompe il ciclo e memorizza il delimitatore corretto per le fasi successive.
	\end{itemize}
	
	\item \textbf{Gestione di possibili errori di encoding}
	\begin{itemize}
		\item I file CSV possono essere salvati con differenti codifiche (\texttt{UTF-8}, \texttt{ISO-8859-1}, \texttt{latin1}, \texttt{cp1252}, ecc.), e una codifica errata può causare errori nella lettura.
		\item Viene tentata la lettura con \texttt{UTF-8}, ma se fallisce, vengono provate altre codifiche.
		\item Se tutte le codifiche testate falliscono, viene segnalato un errore.
	\end{itemize}
	
	\item \textbf{Verifica della struttura del DataFrame}
	\begin{itemize}
		\item Dopo aver letto il file, vengono stampate le prime righe per confermare che i dati siano stati caricati correttamente.
		\item Viene controllato se il dataset contiene colonne indesiderate, come \texttt{Unnamed: X}, che potrebbero derivare da errori nel file originale.
		\item Se esistono colonne numeriche con dati misti (numeri e stringhe), viene segnalato un avviso.
	\end{itemize}
	
	\item \textbf{Conversione dei dati al formato corretto}
	\begin{itemize}
		\item Le colonne numeriche (\texttt{Open}, \texttt{High}, \texttt{Low}, \texttt{Close}) vengono convertite in formato numerico, forzando la conversione (\texttt{errors='coerce'}), in modo da trasformare eventuali valori errati in \texttt{NaN} (valori mancanti).
		\item Se una colonna essenziale come \texttt{DateTime} esiste, viene convertita in un formato di data/ora (\texttt{datetime64}).
		\item Se alcune righe contengono errori di conversione (es. date errate o valori non numerici), vengono rimosse.
	\end{itemize}
\end{enumerate}
Questa fase assicura che il file venga letto in modo robusto e senza errori, adattando la configurazione al formato effettivo del CSV. Il risultato finale è un dataset pulito e ben strutturato, pronto per ulteriori operazioni di analisi e modellazione.\\
Nel terminale visualizzeremo:\\

\subsection*{Correzione e Preprocessing dei Dati}

\noindent\rule{\textwidth}{0.4pt}

\textbf{Impossibile convertire 'DateTime'}: il valore "13/01/2020 01:15:00 +01:00" non corrisponde al formato \texttt{"\%m/\%d/\%Y \%H:\%M:\%S \%z"} alla posizione 8672.  
Possibili soluzioni:
\begin{itemize}
	\item Usare \texttt{format} se le stringhe hanno un formato coerente.
	\item Usare \texttt{format="ISO8601"} se le stringhe sono tutte in ISO8601 ma non esattamente nello stesso formato.
	\item Usare \texttt{format="mixed"} per inferire automaticamente il formato per ogni elemento individualmente.
\end{itemize}

\textbf{Tentativo di correzione del formato della data:}  
Esempi di date convertite:
\begin{verbatim}
	'02/01/2020 01:15:00 +01:00', '02/01/2020 01:16:00 +01:00', 
	'02/01/2020 01:17:00 +01:00', '02/01/2020 01:18:00 +01:00'
\end{verbatim}

\subsection*{Conversione colonne di prezzo}

Colonne convertite: \texttt{Open}, \texttt{High}, \texttt{Low}, \texttt{Close}  
Tipi di dati iniziali:
\begin{verbatim}
	Open      object
	High      object
	Low       object
	Close     object
	dtype: object
\end{verbatim}

Le seguenti colonne sono state convertite correttamente:
\begin{itemize}
	\item Colonna \texttt{Open}: convertita da formato con virgole
	\item Colonna \texttt{High}: convertita da formato con virgole
	\item Colonna \texttt{Low}: convertita da formato con virgole
	\item Colonna \texttt{Close}: convertita da formato con virgole
\end{itemize}

\textbf{Rimosse 0 righe con valori mancanti}

\subsection*{Statistiche sui dati numerici}

\begin{table}[h]
	\centering
	\begin{tabular}{lcccc}
		\hline
		& Open & High & Low & Close \\
		\hline
		count & 616397 & 616397 & 616397 & 616397 \\
		mean  & 13884.68 & 13887.52 & 13881.79 & 13884.66 \\
		std   & 1747.75  & 1763.92  & 1733.78  & 1747.75 \\
		min   & 7970.00  & 7970.00  & 7970.00  & 7970.00 \\
		25\%  & 12878.50 & 12878.50 & 12878.50 & 12878.50 \\
		50\%  & 13914.00 & 13914.00 & 13914.00 & 13914.00 \\
		75\%  & 15487.00 & 15487.00 & 15487.00 & 15487.00 \\
		max   & 16294.00 & 16294.00 & 16294.00 & 16294.00 \\
		\hline
	\end{tabular}
	\caption{Statistiche descrittive delle colonne numeriche}
\end{table}

\subsection*{Identificazione degli outlier}

\begin{itemize}
	\item Colonna \texttt{Open}: 6270 potenziali outlier (1.02\%)
	\item Colonna \texttt{High}: 6232 potenziali outlier (1.01\%)
	\item Colonna \texttt{Low}: 6331 potenziali outlier (1.03\%)
	\item Colonna \texttt{Close}: 6287 potenziali outlier (1.02\%)
\end{itemize}

\subsection*{Impostazione indice e salvataggio}

La colonna \texttt{DateTime} è stata impostata come indice.  
I dati puliti sono stati salvati nel file \texttt{DAX\_cleaned.csv}.

\subsection*{Visualizzazione dei dati puliti}

\begin{table}[h]
	\centering
	\begin{tabular}{lcccccc}
		\hline
		DateTime & Open & High & Low & Close & Unnamed: 5 \\
		\hline
		02/01/2020 01:15:00 +01:00 & 13174.0 & 13194.5 & 13171.5 & 13177.5 & NaN \\
		02/01/2020 01:15:00 +01:00 & 13174.0 & 13194.5 & 13171.5 & 13177.5 & NaN \\
		02/01/2020 01:16:00 +01:00 & 13177.0 & 13185.0 & 13171.0 & 13180.5 & NaN \\
		02/01/2020 01:16:00 +01:00 & 13177.0 & 13185.0 & 13171.0 & 13180.5 & NaN \\
		02/01/2020 01:17:00 +01:00 & 13180.5 & 13181.5 & 13181.5 & 13182.5 & NaN \\
		02/01/2020 01:16:00 +01:00 & 13177.0 & 13185.0 & 13171.0 & 13180.5 & NaN \\
		02/01/2020 01:16:00 +01:00 & 13177.0 & 13185.0 & 13171.0 & 13180.5 & NaN \\
		02/01/2020 01:16:00 +01:00 & 13177.0 & 13185.0 & 13171.0 & 13180.5 & NaN \\
		02/01/2020 01:15:00 +01:00 & 13174.0 & 13194.5 & 13171.5 & 13177.5 & NaN \\
		\hline
	\end{tabular}
	\caption{Dati puliti dopo il preprocessing}
\end{table}

\subsection*{Visualizzazione grafica dei dati puliti}
Per comodità mostriamo solo il prezzo delle Open.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.1\textwidth]{Pictures/price_chart.png}
	\caption{Ultimi 100 valori di prezzo}
	\label{fig:price_chart}
\end{figure}

\subsection{Stima dei Parametri per Diverse Copule}





















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\nocite{*}  % Mostra tutte le referenze, anche se non citate nel testo
\bibliographystyle{plainnat}  % Stile della bibliografia
\bibliography{references}  % Nome del file .bib senza estensione




\end{document}

